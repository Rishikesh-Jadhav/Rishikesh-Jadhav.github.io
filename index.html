<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Massively by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<div id="intro">
						<h1>This is<br />
						Rishikesh</h1>
						<p>Explore my projects and skills to get to know me better</p>
						<ul class="actions">
							<li><a href="#header" class="button icon solid solo fa-arrow-down scrolly">Continue</a></li>
						</ul>
					</div>

				<!-- Header -->
					<header id="header">
						<a href="index.html" class="logo">Rishikesh's Portfolio</a>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="generic.html">About</a></li>
							<li class="active"><a href="index.html">Portfolio</a></li>
							<li><a href="elements.html">Resume</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://www.linkedin.com/in/rishikesh-avinash-jadhav/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="https://github.com/Rishikesh-Jadhav" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://www.youtube.com/@rishikeshjadhav5295" class="icon brands fa-youtube"><span class="label">YouTube</span></a></li>

						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<!-- Featured Post -->
							<article class="post featured">
								<header class="major">
									<!-- <span class="date">April 25, 2017</span> -->
									<h2><a href="https://github.com/Rishikesh-Jadhav/Analysis-of-Motion-Prediction-for-Autonomous-Driving-using-Multi-Camera-Setup">Discover my latest project <br />
									</a></h2>
									<p>Explore the application of PowerBEV, a robust yet lightweight framework for instance prediction in a bird's-eye view on Toyota's Woven Dataset. This project introduces a novel feature extractor to the architecture, demonstrating enhanced performance. Additionally, the results gathered by the implementation from scratch on Woven are meticulously compared with the NuScenes dataset, showcasing the framework's effectiveness in real-world scenarios and also sub optimal generalizability on other datasets.</p>
								</header>
								<a href="https://github.com/Rishikesh-Jadhav/Analysis-of-Motion-Prediction-for-Autonomous-Driving-using-Multi-Camera-Setup" class="image main"><img src="images/PowerBev2.jpg" alt="" /></a>
								<ul class="actions special">
									<li><a href="https://github.com/Rishikesh-Jadhav/Analysis-of-Motion-Prediction-for-Autonomous-Driving-using-Multi-Camera-Setup" class="button large">Check it out!</a></li>
								</ul>
							</article>

						<!-- Featured Post -->
						<article class="post featured">
							<header class="major">
								<!-- <span class="date">April 25, 2017</span> -->
								<h2><a href="https://github.com/Rishikesh-Jadhav/ARIAC-AgileRoboticsforIndustrialAutomationCompetition2023">Agile Robotics for Inustrial Automation Competition<br />
								</a></h2>
								<p>Explore our team's journey in the ARIAC (Agile Robotics for Industrial Automation competition) 2023, where we engaged in simulated manufacturing tasks to push the boundaries of robotic systems. The competition, hosted by NIST, assessed our capabilities in handling challenges like material assembly, quality inspection, and process optimization in a ROS2 and Gazebo simulated environment.Our Team showcased innovative solutions, including efficient kitting, dynamic pick and place on and from the conveyor belt, precise assembly, and addressing agility challenges such as High Priority orders, Defective parts, Insufficient Orders, Flipped Parts and Faulty Gripper. The project's architecture is centered around pick-and-place tasks, utilizing cameras and sensors for detection, resulting in a successful demonstration of the ARIAC.</p>
							</header>
							<a href="https://github.com/Rishikesh-Jadhav/ARIAC-AgileRoboticsforIndustrialAutomationCompetition2023" class="image main"><img src="images/ariac.png" alt="" /></a>
							<ul class="actions special">
								<li><a href="https://github.com/Rishikesh-Jadhav/ARIAC-AgileRoboticsforIndustrialAutomationCompetition2023" class="button large">Check it out!</a></li>
							</ul>
						</article>							
						
							<!-- Separate section for the 3D Vision Projects title -->
							<section id="vision-projects-title">
								<h1>Robotics Projects</h1>
							</section>
		
							<!-- Posts -->
							<section class="posts">
								<article>
									<header>
										<!-- <span class="date">April 24, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/Mobile-Manipulator-Robot-modeling-and-simulation-using-Gazebo-ROS-Noetic-">9DOF Mobile Manipulator Modeling and simulation for Trash Pickup <br />
										</a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/Mobile-Manipulator-Robot-modeling-and-simulation-using-Gazebo-ROS-Noetic-" class="image fit"><img src="images/robot_manip.png" alt="" /></a>
									<p>Developed a versatile 9-degree-of-freedom mobile manipulator robot in Gazebo, designed for exploration, object manipulation, and pick-and-place tasks. The robot showcased its capability to perform tasks such as cleaning larger objects that a Roomba cannot handle effectively. The project involved validating the forward and inverse kinematics of the robot, which includes a differential drive mobile base and a robotic arm. The closed-loop control mechanism, along with Gazebo and RViz visualizations, demonstrated the robot's effective navigation and pick-and-place operations. Despite encountering challenges, the project successfully laid the foundation for future advancements in this space.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/Mobile-Manipulator-Robot-modeling-and-simulation-using-Gazebo-ROS-Noetic-/blob/main/Report/PROJECT-2%20ENPM662.pdf" class="button">Project Report</a></li>
									</ul>
								</article>

								<article>
									<header>
										<!-- <span class="date">April 22, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/Enhanced-Path-Planning-through-Comparative-Analysis-of-RRT-and-Its-Variants-on-TurtleBot-in-Gazebo">Enhanced Path Planning using Turtlebot with RRT and its Variants<br />
										</a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/Enhanced-Path-Planning-through-Comparative-Analysis-of-RRT-and-Its-Variants-on-TurtleBot-in-Gazebo" lass="image fit"><img src="images/rrt_final.png" alt="" width="420" /></a>
									<p>This project explores enhanced path planning for TurtleBot in 2D and 3D environments through a comparative analysis of RRT and its variants. The implemented RRT*, RRT*-Smart, and RRT-Connect algorithms are evaluated and compared for efficient navigation. The project provides simulation results in Gazebo, showcasing the execution of each algorithm. The repository includes instructions for running 2D path planning algorithms and 3D simulation and visualization  in Gazebo and Rviz respectively. Valuable insights into the strengths and weaknesses of RRT variants are presented, contributing to optimized path planning strategies for TurtleBot.</p>
									<ul class="actions special">
										<li><a href="https://drive.google.com/drive/folders/1U-uNljfeKTL3TyrP1F474mEE-5gczQJn" class="button">Project Report</a></li>
									</ul>
								</article>

								<article>
									<header>
										<!-- <span class="date">April 18, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/Simulation-and-Technical-Report-on-Adaptive-control-for-UAVs-equipped-with-a-robotic-arm">Simulation of Adaptive control for UAVs eqipped with a robotic arm<br />
										</a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/Simulation-and-Technical-Report-on-Adaptive-control-for-UAVs-equipped-with-a-robotic-arm" class="image fit"><img src="images/drone.png" alt="" /></a>
									<p>This project presents a comprehensive technical report and simulations on adaptive control for Unmanned Aerial Vehicles (UAVs) equipped with a 5 DOF robotic arm. The motion control challenges of an end effector attached to a quadrotor are addressed through a hierarchical control architecture, emphasizing control schemes and stability analysis. The outputs showcase the end effector trajectory, control architecture block scheme, position and rotation error analysis, and the end trajectory. </p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/Simulation-and-Technical-Report-on-Adaptive-control-for-UAVs-equipped-with-a-robotic-arm/blob/main/Technical_Report.pdf" class="button">Project Report</a></li>
									</ul>								
								</article>

								<article>
									<header>
										<!-- <span class="date">April 14, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/F1-Car-CAD_Modeling-and-Simulation-in-Gazebo">F1 Isnpired Car Modeling and Simulation <br />
										 </a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/F1-Car-CAD_Modeling-and-Simulation-in-Gazebo" class="image fit"><img src="images/f1_car_cad_gazebo.png" alt="" /></a>
									<p>This project involves the CAD modeling of a Formula 1 car in Solidworks, its conversion to a URDF file, and subsequent simulation and visualization with attached sensors in Gazebo and Rviz respectively. The goals include adding Lidar sensor modules, creating a competition arena, and implementing TeleOP for navigation. The project flow covers creating part models, assembling the car, exporting as URDF, configuring controllers, and integrating Lidar sensors. The Simulation videos showcase TeleOP and Publisher-Subscriber functionalities. </p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/F1-Car-CAD_Modeling-and-Simulation-in-Gazebo/blob/main/Report/report.pdf" class="button">Project Report</a></li>
									</ul>
								</article>

								<article>
									<header>
										<!-- <span class="date">April 14, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/Turtlebot3-siulation-in-Gazebo-using-ROS2">TurtleBot navigation using Fiducial Makrers<br />
										 </a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/Turtlebot3-siulation-in-Gazebo-using-ROS2" class="image fit"><img src="images/turtlebot.png" alt="" /></a>
									<p>Developed a project, centered around the simulation of TurtleBot3 in Gazebo. The project leveraged fiducial markers for precise navigation and reaching the desired location. Utilizing ROS2, the simulation involved writing a broadcaster, locating fiducial markers, and executing movement to the goal position and correct orientation. </p>
									<ul class="actions special">
										<li><a href="https://drive.google.com/drive/folders/1KIveQKddSs1aFvB65hbDv4E2CBQ8Tp3M" class="button">Project Report</a></li>
									</ul>
								</article>
							</section>						
						
							
							<!-- Separate section for the 3D Vision Projects title -->
							<section id="vision-projects-title">
								<h1>CV DL and ML Projects</h1>
							</section>

							<!-- Posts -->
							<section class="posts">
								
								<article>
									<header>
										<!-- <span class="date">April 24, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/Adaptive-Neural-Network-Based-Control-of-Autonomous-car-in-AirSim">Adaptive Neural Network based Control of Autonomous car <br />
										</a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/Adaptive-Neural-Network-Based-Control-of-Autonomous-car-in-AirSim" class="image fit"><img src="images/airsim.jpg" alt="" /></a>
									<p>This project enhances autonomous vehicle control by integrating imitation learning with Model Predictive Control (MPC) within the AirSim simulator. We use MPC controller to collect data and train the model with an adaptive neural networks, improving steering, throttle and braking inputs. The methodology includes experiments with LIDAR-based obstacle detection, waypoint-driven scenarios, and tries to addresses challenges like the simulation-to-reality gap. While showcasing results, the project acknowledges limitations and provides valuable insights for ongoing research in deep learning for autonomous vehicles.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/Adaptive-Neural-Network-Based-Control-of-Autonomous-car-in-AirSim/blob/main/CVPR_Report.pdf" class="button">Project Report</a></li>
									</ul>
								</article>
								
								<article>
									<header>
										<!-- <span class="date">April 22, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/Car-Velocity-Estimation-using-OpticalFlow-DL">Car Velocity Estimation Using Optical Flow<br />
										</a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/Car-Velocity-Estimation-using-OpticalFlow-DL" class="image fit"><img src="images/optical flow project.png" alt="" height="430"/></a>
									<p>This project integrates the optimal optical flow approach, determined through a comparison of classic algorithms (Lucas-Kanade, Farneback) and the advanced deep learning method RAFT (Recurrent All-Pairs Fields Transforms). The selected optical flow method is combined with YOLO (vehicle detection) contributing to vehicle speed estimation. Deployed on a Raspberry Pi 4-based robot rover, the project showcases real-world applicability, with certain limitations. </p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/Adaptive-Neural-Network-Based-Control-of-Autonomous-car-in-AirSim/blob/main/CVPR_Report.pdf" class="button">Project Report</a></li>
									</ul>
								</article>
								
								<article>
									<header>
										<!-- <span class="date">April 18, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/Food-Classification-App">Food Classification App <br />
										</a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/Food-Classification-App" class="image fit"><img src="images/food_app.png" alt="" /></a>
									<p>In this project, I employed the EffnetB2 and ViT base 16 models for efficient feature extraction. These feature extractors underwent training on the foodvision101 dataset. Following the training, I examined the loss curves and stored feature extractor statistics for both models. Subsequently, I conducted a comprehensive comparison of the model results, prediction times during inference, and model sizes to identify the best-performing feature extractor. The chosen model was then utilized to develop the application demo using Gradio, ensuring best practices for packaging files. The final step encompassed deploying the application on Hugging Face spaces.</p>
									<ul class="actions special">
										<li><a href="https://huggingface.co/spaces/Rishikesh22/Food_classification" class="button">App Demo</a></li>
									</ul>
								</article>
								
								<article>
									<header>
										<!-- <span class="date">April 14, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/CMSC828I-Advanced-Techniques-in-Visual-Learning-Recognition">Superpixels and Image segmentation<br />
										 </a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/CMSC828I-Advanced-Techniques-in-Visual-Learning-Recognition" class="image fit"><img src="images/superpix.png" alt="" /></a>
									<p>Implemented classical and deep learning approaches for superpixels and image segmentation in this project, utilizing the MSRC Object Recognition Image Database. Superpixel segmentation was carried out through Kmeans clustering and the SLIC algorithm from scratch. A superpixel patch dataset was created from the MSRC dataset by applying SLIC to it, and the data was prepared for segmentation tasks. ResNet50 was employed for feature extraction, LR scheduling was applied and the model was trained, evaluated, and fine-tuned. Outputs were visualized and advanced techniques like feature fusion were utilized. This project demonstrates versatility in handling custom datasets, ensuring efficient data processing, and exploring enhancements for improved model performance.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/CMSC828I-Advanced-Techniques-in-Visual-Learning-Recognition/blob/main/Project1/rjadhav1(119256534)cmsc828I_fall2023_HW1.ipynb" class="button">Notebook Link</a></li>
									</ul>
								</article>
								
								<article>
									<header>
										<!-- <span class="date">April 14, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/CMSC828I-Advanced-Techniques-in-Visual-Learning-Recognition">Implicit Neural Representations <br />
										 </a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/CMSC828I-Advanced-Techniques-in-Visual-Learning-Recognition" class="image fit"><img src="images/inr_new.png" alt="" /></a>
									<p>In this project, I explored Implicit Neural Representation (INR) with hands-on experience of parameterizing signals with a neural network. Implemented a SingleImageDataset for image processing, defined a feedforward neural network (FFN) tailored for INR and configured the model training setup with criteria (pixel-wise MSE loss), an optimizer, and LR scheduler. Trained the network visualized the loss plots. Reconstructed images using the trained model and computed PSNR for accuracy assessment. Additionally outpainted the image by predicting 20 pixels beyond the original image boundaries.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/CMSC828I-Advanced-Techniques-in-Visual-Learning-Recognition/blob/main/Project2/Rishikesh_cmsc828I_fall_2023_HW2.ipynb" class="button">Notebook Link</a></li>
									</ul>
								</article>
						
								<article>
									<header>
										<!-- <span class="date">April 14, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/ENPM809K-Fundamentals-for-Artificial-Intelligence-and-Deep-Learning">Image Captioning using RNNs, LSTMs, Transformers<br />
										 </a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/ENPM809K-Fundamentals-for-Artificial-Intelligence-and-Deep-Learning" class="image fit"><img src="images/image_cap.png" alt="" /></a>
									<p>This project covers various aspects, starting with Image Captioning using Vanilla RNNs, progressing to Image Captioning with LSTMs, and further exploring Network Visualization techniques such as Saliency maps, Class Visualization, and Fooling Images. The tasks range from addressing challenges in caption generation with Vanilla RNNs to achieving enhanced results with LSTMs. Additionally, the project provides implementation of a Transformer decoder for image captioning tasks. </p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/ENPM809K-Fundamentals-for-Artificial-Intelligence-and-Deep-Learning/blob/main/Assignment_3.pdf" class="button">Notebook Link</a></li>
									</ul>
								</article>								
						
								<article>
									<header>
										<!-- <span class="date">April 14, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/ENPM673-Perception-for-Autonomous-Robots">Stereo Vision and Depth Perception<br />
										 </a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/ENPM673-Perception-for-Autonomous-Robots" class="image fit"><img src="images/pic05.jpg" alt="" /></a>
									<p>
										Implemented a comprehensive pipeline for Stereo Vision and Depth Perception. This project involves tasks such as camera calibration, rectification, correspondence, and depth computation. Using ORB feature extraction, I estimated Fundamental and Essential matrices, achieving translation and rotation decompositions. The rectification pipeline employed perspective transformation and homography matrices, visualizing effects through epipolar lines. The correspondence pipeline featured matching windows and disparity calculation, generating disparity heat maps. Lastly, the depth computation pipeline calculated depth values, providing grayscale and color heat maps for visualization. </p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/ENPM673-Perception-for-Autonomous-Robots/blob/main/project4/rjadhav1_proj4.pdf" class="button">Project Report</a></li>
									</ul>
								</article>
								
								<article>
									<header>
										<!-- <span class="date">April 14, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/ENPM673-Perception-for-Autonomous-Robots">Camera Pose Estimation and Image Stitching<br />
										 </a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/ENPM673-Perception-for-Autonomous-Robots" class="image fit"><img src="images/cam_pose.png" alt="" width="450" /></a>
									<p>Implemented Camera Pose Estimation and Image Stitching using classic computer vision methods from scratch. Homography for camera pose estimation, involving an image processing pipeline with steps like Hough transform and decomposition for rotation and translation. Additionally, performed image stitching to create a panoramic view by extracting features, matching, and applying homographies for seamless blending. Overcame challenges in edge detection, rotation, translation, and stitching, showcasing problem-solving skills in computer vision. </p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/ENPM673-Perception-for-Autonomous-Robots/blob/main/project2/rjadhav1_proj2.pdf" class="button">Project Report</a></li>
									</ul>
								</article>
								

								
							</section>


							<!-- Separate section for the 3D Vision Projects title -->
							<section id="vision-projects-title">
								<h1>3D Vision Projects</h1>
							</section>
							<!-- Posts -->
							<section class="posts">
								<article>
									<header>
										<!-- <span class="date">April 18, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/3D-Indoor-Mapping-and-Object-Segmentation">3D Indoor Mapping and Segmentation<br />
										</a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/3D-Indoor-Mapping-and-Object-Segmentation" class="image fit"><img src="images/room_seg.png" alt="" /></a>
									<p>In the world of 3D Indoor Mapping and Object Segmentation, where RGB images fuel a cutting-edge approach for autonomous robot navigation in complex Environments. This project integrates SimpleRecon and Point-Voxel CNN, employing depth maps, 3D reconstruction, and point cloud segmentation. Leveraging state-of-the-art technologies like Infinitam and Open3D, our methodology transforms indoor environments into efficient 3D representations. The project showcases a fusion of advanced mapping techniques and neural network segmentation models contributing to the indoor robotic navigation space.</p>
									<ul class="actions special">
										<li><a href="https://github.com/Rishikesh-Jadhav/3D-Indoor-Mapping-and-Object-Segmentation/blob/main/848f_final_report.pdf" class="button">Project Report</a></li>
									</ul>
								</article>

								<article>
									<header>
										<!-- <span class="date">April 14, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision">Point Cloud Segmentation and Classification<br />
										 </a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision" class="image fit"><img src="images/class_and_seg_3d.png" alt="" /></a>
									<p>This project unfolds insights into implementing PointNet-based architectures for both classification and segmentation tasks. Defined Models to handle input points from diverse classes and explored model initialization, training, and testing procedures. PointNet was applied in segmentation tasks, and robustness analysis experiments were performed to evaluate the model's performance under different conditions.</p>
									<ul class="actions special">
										<li><a href="3D-Vision/Assignment4_webpage_write_up/starter.md.html" class="button">Project Report</a></li>
									</ul>
								</article>
								
								<article>
									<header>
										<!-- <span class="date">April 18, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision">Volume Rendering and Neural Radiance Fields<br />
										</a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision" class="image fit"><img src="images/nerf_new.png" alt="" /></a>
									<p>This project involves differentiable volume rendering, including ray and point sampling, volume rendering techniques, and the emission-absorption model. Optimization includes ray sampling, loss functions, and training strategies for both basic implicit volumes and Neural Radiance Fields. The implementation of NeRF as a Multi-Layer Perceptron, loss formulation for RGB image training, and visualization techniques further enhance the project.</p>
									<ul class="actions special">
										<li><a href="../3D-Vision/Assignment3_webpage_write_up/starter.md.html" class="button">Project Report</a></li>
									</ul>
								</article>

								<article>
									<header>
										<!-- <span class="date">April 22, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision">Single View to 3D<br />
										</a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision" class="image fit"><img src="images/single_2_3d.png" alt="" /></a>
									<p> This Project focuses on single-view to 3D transformation. It highlights the design of neural networks for image-to-voxel grid, image-to-point cloud, and image-to-mesh models. Binary cross-entropy and Chamfer loss functions for voxel grids and point clouds, respectively. The inclusion of quantitative comparisons, hyperparameter analyses, and insightful visualizations provides a thorough understanding of the application.</p>
									<ul class="actions special">
										<li><a href="../3D-Vision/Assignment2_webpage_write_up/starter.md.html" class="button">Project Report</a></li>
									</ul>
								</article>
								
								<article>
									<header>
										<!-- <span class="date">April 24, 2017</span> -->
										<h2><a href="https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision">Rendering with Pytorch3D<br />
										</a></h2>
									</header>
									<a href="https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision" class="image fit"><img src="images/rendering.png" alt="" /></a>
									<p>In this project, mesh rendering, camera manipulation for 360-degree gifs, implementation of the Dolly Zoom effect, creation and rendering of 3D shapes, re-texturing of meshes based on vertex positions, camera pose transformations and additionally, rendering generic 3D representations is implemented. The project also explores implicit surfaces, employing implicit functions to define surfaces and discussing the trade-offs between mesh and point cloud rendering.</p>
									<ul class="actions special">
										<li><a href="../3D-Vision/Assignment1_webpage_write_up/starter.md.html" class="button">Project Report</a></li>
									</ul>
								</article>



							</section>	
							
						<!-- Footer -->
							<!-- <footer> -->
								<!-- <div class="pagination"> -->
									<!--<a href="#" class="previous">Prev</a>-->
									<!-- <a href="#" class="page active">1</a>
									<a href="#" class="page">2</a>
									<a href="#" class="page">3</a>
									<span class="extra">&hellip;</span>
									<a href="#" class="page">8</a>
									<a href="#" class="page">9</a>
									<a href="#" class="page">10</a>
									<a href="#" class="next">Next</a> -->
								<!-- </div> -->
							<!-- </footer> -->

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<!-- <form method="post" action="#">
								<div class="fields">
									<div class="field">
										<label for="name">Name</label>
										<input type="text" name="name" id="name" />
									</div>
									<div class="field">
										<label for="email">Email</label>
										<input type="text" name="email" id="email" />
									</div>
									<div class="field">
										<label for="message">Message</label>
										<textarea name="message" id="message" rows="3"></textarea>
									</div>
								</div>
								<ul class="actions">
									<li><input type="submit" value="Send Message" /></li>
								</ul>
							</form>
						</section>
						<section class="split contact">
							<section class="alt">
								<h3>Address</h3>
								<p>1234 Somewhere Road #87257<br />
								Nashville, TN 00000-0000</p>
							</section>
							<section>
								<h3>Phone</h3>
								<p><a href="#">(000) 000-0000</a></p>
							</section> -->
							<section>
								<h3>Email</h3>
								<p><a href="#">rjadhav1@umd.edu</a></p>
								<p><a href="#">rishikeshjadhav712@gmail.com</a></p>
							</section>
							<section>
								<h3>Social</h3>
								<ul class="icons alt">
									<li><a href="https://www.linkedin.com/in/rishikesh-avinash-jadhav/" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
									<li><a href="https://github.com/Rishikesh-Jadhav" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
									<li><a href="https://www.youtube.com/@rishikeshjadhav5295" class="icon brands alt fa-youtube"><span class="label">YouTube</span></a></li>
								</ul>
							</section>
						</section>
					</footer>

				<!-- Copyright -->
					<div id="copyright">
						<ul><li>&copy; Untitled</li><li>Design: <a href="https://html5up.net">HTML5 UP</a></li></ul>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>